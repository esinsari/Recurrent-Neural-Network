{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LwvvMtG18SK"
      },
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "Implement a basic RNN network and an LSTM network with Keras\\\n",
        "Use Machine Learning libaries like Scikit-learn for data preprocessing.\n",
        "\n",
        "**Task Overview:**\n",
        "- Implement a basic RNN network to solve time series prediction \n",
        "- Implement an LSTM network to conduct sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24oSrIK18SL"
      },
      "source": [
        "## 1 - Implement Basic RNN network with Keras to predict time series##\n",
        "### 1.1 Prepare the data\n",
        "\n",
        "Prepare time series data for deep neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load train and test data: \"train.txt\" and \"test.txt\".\n",
        "2. Generate the **TRAIN** and **TEST** labels. \n",
        "2. Normalize the **TRAIN** and **TEST** data with sklearn function \"MinMaxScaler\". \n",
        "3. Print out the **TEST** data and label.\n",
        "\n",
        "\n",
        "Apply the MinMaxScaler to both the train and test data.\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS7xhrC3-_-1"
      },
      "source": [
        "### Import Libraries ###\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers \n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "reset_random_seeds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shAj2Y6IuxUv"
      },
      "source": [
        "### Prepare and Preprocess Data Here ###\n",
        "\n",
        "from pandas import read_csv\n",
        "\n",
        "### Design a Function to Prepare Observation Sequences and Corresponding Labels ###\n",
        "def create_dataset(dataset, look_back=12): # look_back is used to specify input sequence length\n",
        "     dataX, dataY = [], []\n",
        "     for i in range(len(dataset)-look_back):\n",
        "         dataX.append(dataset[i:i + look_back]) # make sure correct start and end elements\n",
        "         dataY.append(dataset[i + look_back]) # make sure correct start and end elements; here, we have only one point ahead for prediction\n",
        "     return np.array(dataX), np.array(dataY)\n",
        "\n",
        "### Train and Test Data Loading with float32 type ####\n",
        "dataframe_test = read_csv('test.txt')['Passengers']\n",
        "dataset_test = dataframe_test.values\n",
        "dataset_test = dataset_test.astype('float32')\n",
        "test_data = dataset_test.reshape(len(dataset_test), 1)\n",
        "\n",
        "dataframe_train = read_csv('train.txt')['Passengers']\n",
        "dataset_train = dataframe_train.values\n",
        "dataset_train = dataset_train.astype('float32')\n",
        "train_data = dataset_train.reshape(len(dataset_train), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8MlgYTvvIeD"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "### Scale Training and Test Data to [0, 1] ###\n",
        "scaler = MinMaxScaler(feature_range=(0, 1)) # specify the scaler\n",
        "train = scaler.fit_transform(train_data) # fit the scaler to the training data\n",
        "test = scaler.fit_transform(test_data) # fit the scaler to the test data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFjx9u37jAJR"
      },
      "source": [
        "### Train and Test Data Split\n",
        "trainX = np.expand_dims(train, axis=1)\n",
        "testX = np.expand_dims(test, axis=1)\n",
        "\n",
        "trainX, trainY = create_dataset(train, look_back=12) # historical window is 12; future window is 1.\n",
        "testX, testY = create_dataset(test, look_back=12)\n",
        "\n",
        "### Train and Test Data Reshape (to fit RNN input)\n",
        "\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1])) \n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ1uHWzX8wlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77075c6-52cc-4930-9acf-eb1d6a43ed68"
      },
      "source": [
        "# print out the test data and label here\n",
        "print(testX)\n",
        "print(testY)\n",
        "\n",
        "print(\"TrainX.shape = \", trainX.shape)\n",
        "print(\"TrainY.shape = \", trainY.shape)\n",
        "\n",
        "print(\"TestX.shape = \", testX.shape)\n",
        "print(\"TestY.shape = \", testY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0.5801282  0.625      0.30128205 0.15705132 0.         0.08653843\n",
            "   0.16025639 0.1025641  0.3076923  0.27564108 0.3525641  0.5192307 ]]\n",
            "\n",
            " [[0.625      0.30128205 0.15705132 0.         0.08653843 0.16025639\n",
            "   0.1025641  0.3076923  0.27564108 0.3525641  0.5192307  0.7628205 ]]\n",
            "\n",
            " [[0.30128205 0.15705132 0.         0.08653843 0.16025639 0.1025641\n",
            "   0.3076923  0.27564108 0.3525641  0.5192307  0.7628205  0.798077  ]]\n",
            "\n",
            " [[0.15705132 0.         0.08653843 0.16025639 0.1025641  0.3076923\n",
            "   0.27564108 0.3525641  0.5192307  0.7628205  0.798077   0.49038458]]\n",
            "\n",
            " [[0.         0.08653843 0.16025639 0.1025641  0.3076923  0.27564108\n",
            "   0.3525641  0.5192307  0.7628205  0.798077   0.49038458 0.31089747]]\n",
            "\n",
            " [[0.08653843 0.16025639 0.1025641  0.3076923  0.27564108 0.3525641\n",
            "   0.5192307  0.7628205  0.798077   0.49038458 0.31089747 0.16666663]]\n",
            "\n",
            " [[0.16025639 0.1025641  0.3076923  0.27564108 0.3525641  0.5192307\n",
            "   0.7628205  0.798077   0.49038458 0.31089747 0.16666663 0.30448723]]\n",
            "\n",
            " [[0.1025641  0.3076923  0.27564108 0.3525641  0.5192307  0.7628205\n",
            "   0.798077   0.49038458 0.31089747 0.16666663 0.30448723 0.34294868]]\n",
            "\n",
            " [[0.3076923  0.27564108 0.3525641  0.5192307  0.7628205  0.798077\n",
            "   0.49038458 0.31089747 0.16666663 0.30448723 0.34294868 0.25961542]]\n",
            "\n",
            " [[0.27564108 0.3525641  0.5192307  0.7628205  0.798077   0.49038458\n",
            "   0.31089747 0.16666663 0.30448723 0.34294868 0.25961542 0.34935904]]\n",
            "\n",
            " [[0.3525641  0.5192307  0.7628205  0.798077   0.49038458 0.31089747\n",
            "   0.16666663 0.30448723 0.34294868 0.25961542 0.34935904 0.48397434]]\n",
            "\n",
            " [[0.5192307  0.7628205  0.798077   0.49038458 0.31089747 0.16666663\n",
            "   0.30448723 0.34294868 0.25961542 0.34935904 0.48397434 0.5192307 ]]\n",
            "\n",
            " [[0.7628205  0.798077   0.49038458 0.31089747 0.16666663 0.30448723\n",
            "   0.34294868 0.25961542 0.34935904 0.48397434 0.5192307  0.72115386]]\n",
            "\n",
            " [[0.798077   0.49038458 0.31089747 0.16666663 0.30448723 0.34294868\n",
            "   0.25961542 0.34935904 0.48397434 0.5192307  0.72115386 1.        ]]\n",
            "\n",
            " [[0.49038458 0.31089747 0.16666663 0.30448723 0.34294868 0.25961542\n",
            "   0.34935904 0.48397434 0.5192307  0.72115386 1.         0.94871795]]\n",
            "\n",
            " [[0.31089747 0.16666663 0.30448723 0.34294868 0.25961542 0.34935904\n",
            "   0.48397434 0.5192307  0.72115386 1.         0.94871795 0.6346154 ]]\n",
            "\n",
            " [[0.16666663 0.30448723 0.34294868 0.25961542 0.34935904 0.48397434\n",
            "   0.5192307  0.72115386 1.         0.94871795 0.6346154  0.48397434]]]\n",
            "[[0.7628205 ]\n",
            " [0.798077  ]\n",
            " [0.49038458]\n",
            " [0.31089747]\n",
            " [0.16666663]\n",
            " [0.30448723]\n",
            " [0.34294868]\n",
            " [0.25961542]\n",
            " [0.34935904]\n",
            " [0.48397434]\n",
            " [0.5192307 ]\n",
            " [0.72115386]\n",
            " [1.        ]\n",
            " [0.94871795]\n",
            " [0.6346154 ]\n",
            " [0.48397434]\n",
            " [0.25641024]]\n",
            "TrainX.shape =  (101, 1, 12)\n",
            "TrainY.shape =  (101, 1)\n",
            "TestX.shape =  (17, 1, 12)\n",
            "TestY.shape =  (17, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AlakqA_vuFb"
      },
      "source": [
        "### 1.2 - Build the RNN model ##\n",
        "\n",
        "\n",
        "Build an RNN model with SimpleRNN cell. \n",
        "\n",
        "**Tasks:**\n",
        "1. Build an RNN model with 1 RNN layer and 1 Dense layer.\n",
        "2. Compile the model.\n",
        "3. Train the model for 100 epochs with **batch_size = 10**. \n",
        "\n",
        "**tensorflow.keras.layers.SimpleRNN(unit_size=4)** to specify RNN cells.\n",
        "Loss function = 'mean_squared_error' and select **Adam** optimizer with **learning_rate=0.01** and other default settings.\n",
        "After first epoch, the train loss is changed to around **0.0656**. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn92qh8oyq0B"
      },
      "source": [
        "### Build the RNN Model ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model = Sequential() # Declare Sequential class and assign it to variable \"model\"\n",
        "model.add(tf.keras.layers.SimpleRNN(units=4)) # Add a simple RNN layer with unit_size=4 in the model \n",
        "model.add(keras.layers.Dense(units=1)) # Add a following Dense layer with units=1 in the model \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnO-5WT-3hgH"
      },
      "source": [
        "### Compile the RNN model ###\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='mean_squared_error', optimizer=opt)# model compiled with mean_squared_error loss and adam optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tpZAutlzify",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d91c810d-021f-4641-d8ac-c4adc53421a8"
      },
      "source": [
        "### Train the RNN model and PRINT OUT MODEL STRUCTURE with model.summary() ###\n",
        "\n",
        "model.fit(x=trainX, y=trainY, epochs=100, batch_size=10, verbose=2) # model fit with epoch=100, batch_size=10; verbose=2 is optional.\n",
        "model.summary() # print out model structure with model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 - 0s - loss: 0.0656\n",
            "Epoch 2/100\n",
            "11/11 - 0s - loss: 0.0340\n",
            "Epoch 3/100\n",
            "11/11 - 0s - loss: 0.0215\n",
            "Epoch 4/100\n",
            "11/11 - 0s - loss: 0.0144\n",
            "Epoch 5/100\n",
            "11/11 - 0s - loss: 0.0125\n",
            "Epoch 6/100\n",
            "11/11 - 0s - loss: 0.0115\n",
            "Epoch 7/100\n",
            "11/11 - 0s - loss: 0.0092\n",
            "Epoch 8/100\n",
            "11/11 - 0s - loss: 0.0080\n",
            "Epoch 9/100\n",
            "11/11 - 0s - loss: 0.0068\n",
            "Epoch 10/100\n",
            "11/11 - 0s - loss: 0.0056\n",
            "Epoch 11/100\n",
            "11/11 - 0s - loss: 0.0047\n",
            "Epoch 12/100\n",
            "11/11 - 0s - loss: 0.0055\n",
            "Epoch 13/100\n",
            "11/11 - 0s - loss: 0.0035\n",
            "Epoch 14/100\n",
            "11/11 - 0s - loss: 0.0035\n",
            "Epoch 15/100\n",
            "11/11 - 0s - loss: 0.0034\n",
            "Epoch 16/100\n",
            "11/11 - 0s - loss: 0.0037\n",
            "Epoch 17/100\n",
            "11/11 - 0s - loss: 0.0028\n",
            "Epoch 18/100\n",
            "11/11 - 0s - loss: 0.0029\n",
            "Epoch 19/100\n",
            "11/11 - 0s - loss: 0.0029\n",
            "Epoch 20/100\n",
            "11/11 - 0s - loss: 0.0029\n",
            "Epoch 21/100\n",
            "11/11 - 0s - loss: 0.0022\n",
            "Epoch 22/100\n",
            "11/11 - 0s - loss: 0.0022\n",
            "Epoch 23/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 24/100\n",
            "11/11 - 0s - loss: 0.0024\n",
            "Epoch 25/100\n",
            "11/11 - 0s - loss: 0.0027\n",
            "Epoch 26/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 27/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 28/100\n",
            "11/11 - 0s - loss: 0.0018\n",
            "Epoch 29/100\n",
            "11/11 - 0s - loss: 0.0018\n",
            "Epoch 30/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 31/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 32/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 33/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 34/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 35/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 36/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 37/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 38/100\n",
            "11/11 - 0s - loss: 0.0018\n",
            "Epoch 39/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 40/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 41/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 42/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 43/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 44/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 45/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 46/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 47/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 48/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 49/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 50/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 51/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 52/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 53/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 54/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 55/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 56/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 57/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 58/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 59/100\n",
            "11/11 - 0s - loss: 0.0022\n",
            "Epoch 60/100\n",
            "11/11 - 0s - loss: 0.0035\n",
            "Epoch 61/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 62/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 63/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 64/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 65/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 66/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 67/100\n",
            "11/11 - 0s - loss: 0.0018\n",
            "Epoch 68/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 69/100\n",
            "11/11 - 0s - loss: 0.0023\n",
            "Epoch 70/100\n",
            "11/11 - 0s - loss: 0.0024\n",
            "Epoch 71/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 72/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 73/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 74/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 75/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 76/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 77/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 78/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 79/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 80/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 81/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 82/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 83/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 84/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 85/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 86/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 87/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 88/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 89/100\n",
            "11/11 - 0s - loss: 0.0011\n",
            "Epoch 90/100\n",
            "11/11 - 0s - loss: 0.0011\n",
            "Epoch 91/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 92/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 93/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 94/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 95/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 96/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 97/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 98/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 99/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 100/100\n",
            "11/11 - 0s - loss: 0.0021\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 4)                 68        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 73\n",
            "Trainable params: 73\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd2jZl4n0H8m"
      },
      "source": [
        "### 1.3 Evaluate Predictive Model Performance\n",
        "\n",
        "Predict datapoints with the observed datapoints and trained model. \n",
        "\n",
        "**Tasks:**\n",
        "1. Do direct prediction on train and test datapoints with the obtained model in section 1.2. \n",
        "2. Scale the prediction results back to original representation with the scaler. \n",
        "3. Calculate root mean squared error (RMSE) and **print out** the error for **both TRAIN and TEST**.\n",
        "4. **Plot** the **TEST** label and prediction. \n",
        "\n",
        " \n",
        "Scale back the predictions with the build-in function \"scaler.inverse_transform\".\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform\n",
        " \n",
        "For validation: Train Score: **~13 RMSE** Test Score: **~19 RMSE**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNEkAMxnz8Mq"
      },
      "source": [
        "### Make Predictions ###\n",
        "\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbnRqEv9z-he"
      },
      "source": [
        "### Scale Back Predictions ###\n",
        "\n",
        "trainPredict = scaler.inverse_transform(trainPredict) # scale train prediction back with scaler.inverse_transform()\n",
        "trainY = scaler.inverse_transform(trainY)  # scale train labels back with scaler.inverse_transform()\n",
        "\n",
        "testPredict = scaler.inverse_transform(testPredict) # scale test prediction back with scaler.inverse_transform()\n",
        "testY = scaler.inverse_transform(testY) # scale test labels back with scaler.inverse_transform()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdBWzmE91G6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69c6cea-6bbe-4b9a-e552-e21abe2f1609"
      },
      "source": [
        "### Calculate Root Mean Squared Error (RMSE) ###\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error # Import mean_squared_error from sklearn.metrics\n",
        "\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[:,0], trainPredict[:,0]))\n",
        "testScore = math.sqrt(mean_squared_error(testY[:,0], testPredict[:,0])) \n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "print('Test Score: %.2f RMSE' % (testScore))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Score: 13.03 RMSE\n",
            "Test Score: 19.02 RMSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txdu8q7l1aju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "bba8a194-79eb-44e4-8b62-ca1c065a69a9"
      },
      "source": [
        "### Plot Baseline and Predictions ###\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(testY) \n",
        "plt.plot(testPredict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6e493116d8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O49Ug-FhtCg8"
      },
      "source": [
        "## 2 - Build an LSTM model to conduct sentiment analysis ##\n",
        "\n",
        "### 2.1 Prepare the data ###\n",
        "\n",
        "Prepare IMDB data for reccurent neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the data from IMDB review dataset and **print out** the lengths of sequences.\n",
        "2. Preprocess review data to meet the network input requirement by specifying **number of words=1000**, setting **the analysis length of the review = 100**, and **padding the input sequences**. \n",
        "\n",
        " \n",
        "  Load the IMDB data with keras.datasets.imdb.load_data(num_words=max_features)  max_features is set to **1000**.\n",
        " \n",
        "  Use keras.preprocessing.sequence.pad_sequences(x_train, maxlen) to pad the input sequences and set maxlen to **100**.\n",
        "\n",
        "**Note:**\\\n",
        "We train the built LSTM-based model with ALL training data; the **validation set** (aka **development set**) is set with the **testing set** for model evaluation. This split is common in the application with limited sampled observation data, like NLP problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI4ki461S2V3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras import layers\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvV1Sv2a18SM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d24f9ea-bb74-482c-8294-1e0f5649a986"
      },
      "source": [
        "# Prepare the data here\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words = 1000) # load IMDB data with specified num_words = 1000; testing set is set to validation set.\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen = 100) # Pad IMDB training data with specified maxlen=100\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen = 100) # Pad IMDB validation data with specified maxlen=100\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JFQeWK18SR"
      },
      "source": [
        "### 2.2 - Design and train LSTM model###\n",
        "\n",
        "Build an LSTM model.\n",
        "\n",
        "**Tasks:**\n",
        "1. Build the LSTM model with **1 embedding layer**, **1 LSTM layer**, and **1 Dense layer**. **Print out** model summary. The embedding vector is specified with the dimension of **8**.\n",
        "2. Compile the LSTM model with **Adam** optimizer, **binary_crossentropy** loss function, and **accuracy** metrics.  \n",
        "3. Train the LSTM model with **batch_size=64 for 10 epochs** and report **training and validation accuracies over epochs**. \n",
        "4. **Print out** best validation accuracy.\n",
        "\n",
        " \n",
        "For validation:\\\n",
        "**loss: ~0.5675 - accuracy: ~0.7072 - val_loss: ~0.4549 - val_accuracy: ~0.8020**\n",
        "\n",
        "\n",
        " https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDqqgFt118SS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8df6ba-0fbd-4430-b581-836c5eeea474"
      },
      "source": [
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 8)(inputs) # Embed data in an 8-dimensional vector\n",
        "x = layers.LSTM(8)(x) # Add 1st layer of LSTM with 8 hidden states (aka units)\n",
        "outputs = layers.Dense(units=1, activation=\"sigmoid\")(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])# Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))# Train the compiled model with model.fit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 8)           8000      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 8)                 544       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 8,553\n",
            "Trainable params: 8,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.5182 - accuracy: 0.7363 - val_loss: 0.4131 - val_accuracy: 0.8140\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 0.3945 - accuracy: 0.8237 - val_loss: 0.3849 - val_accuracy: 0.8263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f109c959e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vqvy2tdEw7J"
      },
      "source": [
        "### 2.3 - LSTM hyperparameter tuning###\n",
        "\n",
        "Boost the performance of obtained LSTM (aka vanilla model) by hyperparameter tuning.\n",
        "\n",
        "**Tasks:**\n",
        "- All modificiations are directly conducted based on the vanilla model.\n",
        "- For each scenario, **report <span style=\"color:red\"> BEST Validation Accuracy </span> and generate Training/Validation <span style=\"color:red\"> Accuracy plots over epochs</span>**. Just paste the plot figures in the cells with **Markdown mode**. \n",
        "1.  Scenario 1:\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 16.\n",
        "    - Modify the units of LSTM to 16.\n",
        "2. Scenario 2:\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "3. Scenario 3:\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "    - Increase analysis length for review data to maxlen = 200\n",
        "\n",
        "\n",
        "For validation:\n",
        "- Scenario 1: **loss: ~0.4968 - accuracy: ~0.7450 - val_loss: ~0.4079 - val_accuracy: ~0.8198**\n",
        "- Scenario 2: **loss: ~0.4764 - accuracy: ~0.7670 - val_loss: ~0.4133 - val_accuracy: ~0.8179**\n",
        "- Scenario 3: **loss: ~0.4819 - accuracy: ~0.7644 - val_loss: ~0.4031 - val_accuracy: ~0.8105**\n",
        "\n",
        "\n",
        "https://keras.io/examples/nlp/bidirectional_lstm_imdb/  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keod5xXkEKnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e80bb8f-9197-408d-f4c8-f28f72b2cc47"
      },
      "source": [
        "########################### Scenario 1 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000  # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 16)(inputs) # Embed data in a 16-dimensional vector\n",
        "x = layers.LSTM(16, return_sequences=True)(x) # Add 1st layer of LSTM with 16 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(16)(x) # Add 2nd layer of LSTM with 16 hidden states (aka units)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"]) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, None, 16)          16000     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, None, 16)          2112      \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 20,241\n",
            "Trainable params: 20,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.4968 - accuracy: 0.7450 - val_loss: 0.4079 - val_accuracy: 0.8198\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.3883 - accuracy: 0.8286 - val_loss: 0.3844 - val_accuracy: 0.8242\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3688 - accuracy: 0.8382 - val_loss: 0.3692 - val_accuracy: 0.8333\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3541 - accuracy: 0.8436 - val_loss: 0.3798 - val_accuracy: 0.8257\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3446 - accuracy: 0.8464 - val_loss: 0.3665 - val_accuracy: 0.8338\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3353 - accuracy: 0.8520 - val_loss: 0.4104 - val_accuracy: 0.8247\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3257 - accuracy: 0.8540 - val_loss: 0.3680 - val_accuracy: 0.8325\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 6s 17ms/step - loss: 0.3188 - accuracy: 0.8594 - val_loss: 0.3659 - val_accuracy: 0.8334\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3110 - accuracy: 0.8602 - val_loss: 0.3800 - val_accuracy: 0.8334\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3048 - accuracy: 0.8656 - val_loss: 0.3928 - val_accuracy: 0.8343\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f109dd3c828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzAE_XjXjAJU",
        "outputId": "8cb142a0-4400-4ef0-b79e-90c5f8c7ed7d"
      },
      "source": [
        "########################### Scenario 2 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen =  100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(128, return_sequences=True)(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(1,activation=\"sigmoid\")(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"]) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 128)         128000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 128)         131584    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 9s 23ms/step - loss: 0.4764 - accuracy: 0.7670 - val_loss: 0.4133 - val_accuracy: 0.8179\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.3825 - accuracy: 0.8285 - val_loss: 0.3706 - val_accuracy: 0.8330\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.3584 - accuracy: 0.8397 - val_loss: 0.3646 - val_accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3322 - accuracy: 0.8540 - val_loss: 0.3648 - val_accuracy: 0.8389\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3159 - accuracy: 0.8638 - val_loss: 0.3551 - val_accuracy: 0.8430\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.2962 - accuracy: 0.8745 - val_loss: 0.3536 - val_accuracy: 0.8445\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.2793 - accuracy: 0.8826 - val_loss: 0.3660 - val_accuracy: 0.8378\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.2637 - accuracy: 0.8912 - val_loss: 0.3778 - val_accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2473 - accuracy: 0.8977 - val_loss: 0.3862 - val_accuracy: 0.8283\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2255 - accuracy: 0.9085 - val_loss: 0.4074 - val_accuracy: 0.8316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1098334860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0PiWALLjAJU",
        "outputId": "e5875443-760b-43da-faf7-cb6ca5986305"
      },
      "source": [
        "########################### Scenario 3 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 200 # Only consider the first 200 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(128, return_sequences=True)(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"]) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 128)         128000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 128)         131584    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.4764 - accuracy: 0.7670 - val_loss: 0.4133 - val_accuracy: 0.8179\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.3825 - accuracy: 0.8285 - val_loss: 0.3706 - val_accuracy: 0.8330\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3584 - accuracy: 0.8397 - val_loss: 0.3646 - val_accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.3322 - accuracy: 0.8540 - val_loss: 0.3648 - val_accuracy: 0.8389\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3159 - accuracy: 0.8638 - val_loss: 0.3551 - val_accuracy: 0.8430\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2962 - accuracy: 0.8745 - val_loss: 0.3536 - val_accuracy: 0.8445\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2793 - accuracy: 0.8826 - val_loss: 0.3660 - val_accuracy: 0.8378\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.2637 - accuracy: 0.8912 - val_loss: 0.3778 - val_accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.2473 - accuracy: 0.8977 - val_loss: 0.3862 - val_accuracy: 0.8283\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.2255 - accuracy: 0.9085 - val_loss: 0.4074 - val_accuracy: 0.8316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1040751828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}